\chapter{Relevant Literature}
\section{Generative Bag-of-Words Topic Modeling}
\subsection{Latent Dirichlet Allocation}
Perhaps first touch upon LSA/(P)LSI.  Explain graphical model and how to train models. What are they most commonly used for in the domain of topic modelling?
\subsection{Deep Exponential Families}
Improvement over LDA. Still with binary latent variables, but now stacked. \\ \\
Put in graphical model of example of a DEF model. \\ \\
Explain training procedure. \\ \\
Sampling procedure pretty expensive and  parameters arent shared so scalability isnt ideal.
\subsection{Other?}
\section{SGVB}
Discuss SGVB in more detail.
\subsection{Applications}
Where has SGVB been used? - images, recurrent versions for sequences (e.g. sentences) etc
\section{Neural Networks approaches to Bag-of-Words data}
Brief overview on papers that use neural networks to model (supervised or unsupervised) bag-of-words data. Discuss general problems that arise and how they have (or have not) been dealt with.
 