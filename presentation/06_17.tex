\documentclass{beamer}

\mode<presentation> {

\usetheme{default}
%\usetheme{Rochester}
%\usecolortheme{lily}

\setbeamertemplate{footline}[page number] 
\beamertemplatenavigationsymbolsempty
\setbeamertemplate{bibliography item}{} %Remove icons in bibliography
}

\usepackage{graphicx} % Allows including images
\usepackage{amsmath}
\usepackage{lmodern}
\usepackage{listings}
\usepackage{hyperref}
\usepackage{wrapfig}



\usepackage{tikz}
\usetikzlibrary{bayesnet}

\lstset{
    language=[5.0]Lua,
    basicstyle=\fontsize{11}{9},
    sensitive=true,
    breaklines=true,
    tabsize=2
}

%----------------------------------------------------------------------------------------
%	TITLE PAGE
%----------------------------------------------------------------------------------------

\title[DEF]{Topic Modelling: Deep Exponential Families} 
\subtitle{relation to SGVB}

\author{Otto Fabius} 
\institute[UvA] 
{University of Amsterdam \\
Supervisor: P.Putzky \\ 
Co-Supervisors: M. Welling, D.P. Kingma
\medskip
}
\date{\today} % Date, can be changed to a custom date

\begin{document}




%----------------------------------------------------------------------------------------
%	PRESENTATION SLIDES
%----------------------------------------------------------------------------------------

\begin{frame}
\frametitle{Graph Convolutions}
\begin{itemize}
\item{Suppose $H = \text{ReLU}(LW)$, with $L={\bar{D}^{-\frac{1}{2}}\bar{A}\bar{D}^{-\frac{1}{2}}}F$ as one layer}
\item{with $F = I$ we can rewrite this as $LW = \bar{X} W_1 + \bar{X}^TW_2$}
\item{$W_2$ now has parameters for each document, leading to problems with both scalability and unseen data.}
\item{Removing (or collapsing) $\bar{X}^TW_2$ leaves us with our original model, but a different normalization $\bar{X}_{ij} = 
\frac{X_{ij}}{\sqrt{\sum_i X_{ij} \sum_{j} X_{ij}}}$}
\item{Could use this for experiments, although theoretically a bit dodgy from a VAE perspective (?)}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Comparison to DEF}
\begin{itemize}
\item{Training on 165000 NYT documents with $V = 8000$, evaluating perplexity on 1000 documents.}
\item{$\text{Perp} =  \frac{1}{\sum\limits_{i=1}^{N}\sum\limits_{k=1}^{K}x_{ik}}\sum\limits_{i=1}^N\sum\limits_{k=1}^{K} \log p(x_{k}|\mathbf{z}_{i}^{s})x_{ik}^{u}$}
\item{LDA: 2717. best DEF: 2251. Best Topic VAE: 2168}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Comparison to DEF - Problems}
\begin{itemize}
\item{We do not have acces to the same NYT documents, although 1000 documents seems enough for a good estimate.}
\item{How many latent variables should we use for a fair comparison? (now 32)}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Perplexity considersations (1)}
\begin{figure}[!tbp]
  \centering
  \begin{minipage}[b]{0.4\textwidth}
    \includegraphics[width=\textwidth]{50_lb.png}
  \end{minipage}
  \hfill
  \begin{minipage}[b]{0.4\textwidth}
    \includegraphics[width=\textwidth]{10_lb.png}
  \end{minipage}
\end{figure}
\begin{figure}[!tbp]
  \centering
  \begin{minipage}[b]{0.4\textwidth}
    \includegraphics[width=\textwidth]{50_perplex.png}
  \end{minipage}
  \hfill
  \begin{minipage}[b]{0.4\textwidth}
    \includegraphics[width=\textwidth]{10_perplex.png}
  \end{minipage}
\end{figure}
\end{frame}


\begin{frame}
\frametitle{Perplexity considersations (2)}
\begin{itemize}
\item{Perplexity becomes a bad evaluation measure (in our case) when using few words for inference.}
\item{This might not be the case for more linear methods}
\item{This also means the lower bound should not be compared to perplexity, as done in Miao et al. "Neural Variational Inference for Text processing." (2016)}
\end{itemize}
To combat this problem, we could consider:
\begin{itemize}
\item{Normalizing according to the total number of words in stead of 10\% during perplexity calculation. This does result in lower values in the first hidden layer, so also doesnt seem great.}
\item{Using dropout in first layer, or more directly: only using ten percent of each document for encoding during training. This trains model w.r.t. evaluation measure more than w.r.t. lowerbound and good inference on whole documents.}
\end{itemize}
\end{frame}

\end{document}