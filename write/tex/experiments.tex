\chapter{Experiments}

The datasets used for our experiments are the KOS blog post entries dataset and the NY Times dataset, both freely available ...footnote uci... bag of words datasets. Preprocessing includes only stopword removal and removal of words that occur ten times or less. After this, the KOS dataset contains 3430 documents and has a vocabulary of 6906 words, and the NY times dataset contains 300,000 documents and has a vocabulary size of 102660. \\
Topic models are evaluated by estimating the heldout perplexity per word on the test set. Perplexity is ..... More specifically, in ur experiments we use half of the words in each document to perform inference, i.e. calculate $p....$, and calculate the perplexity of the other half of the document. This is similar to ..examples..

\section{Bare-bone VAE approach}

Train the model on KOS, compare to LDA. Also train on NY dataset with a limited vocabulary and predict mean for rest of words. \\
Discussion of results includes brief analysis of encoder/decoder structure, overfitting etc.

\section{Random Projections}
Try to improve over baseline by using random projection of infrequent words in the encoder, mainly for NY dataset.

\section{Predicting the full output}
Experiment(s) where the decoder predicts the full vocabulary, limiting the number of parameters used for the infrequent words.

\section{Investigation of the Topic Space}
Perhaps investigate the encoding of two (or more, could even do a whole document) words separately, as well as the combination. How much of an interaction is actually learned? Can we extract which words have a small influence on the encoding? Or which words hardly interact with other words?

